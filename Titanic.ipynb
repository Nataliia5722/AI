{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"Titanic.ipynb\"",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nataliia5722/AI/blob/main/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewd6dVIrJpO_"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQCVsoppJ-Iu"
      },
      "source": [
        "[External data: Local Files, Drive, Sheets, and Cloud Storage](https://colab.research.google.com/notebooks/io.ipynb?authuser=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-fQbctCKDXe"
      },
      "source": [
        "\n",
        "Mounting Google Drive locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNI6gOFZJ_Dt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMr__L8lKJGG"
      },
      "source": [
        "data = pd.read_csv(\"https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/fa71405126017e6a37bea592440b4bee94bf7b9e/titanic.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-qK68zCHRSG"
      },
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3_z7hN4KUMn"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_kNkO1EK-nq"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POe6jC4mLA9O"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Lf44qCLFo6"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hyn6LQKKW5f"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "print(f'matplotlib: {matplotlib.__version__}')\n",
        "print(f'pytorch   : {torch.__version__}')\n",
        "print(f'pandas    : {pd.__version__}')\n",
        "print(f'numpy     : {np.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtLuU6ZqLoNQ"
      },
      "source": [
        "### The formatting\n",
        "\n",
        "- One-hot encode: 'Sex', 'Embarked'\n",
        "- Remove: 'Name', 'Ticket', 'Cabin'\n",
        "- Fill null values with the mean of the associated column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxIhOAAnKnrQ"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def data_normalizer(features):\n",
        "    x = features.values #returns a numpy array\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "    return pd.DataFrame(x_scaled, columns=features.columns)\n",
        "\n",
        "# Apply some data formatting\n",
        "def format_data(data):\n",
        "    # One-hot encode 'Embarked' column\n",
        "    data = pd.get_dummies(data, columns=['Sex','Embarked'])\n",
        "    # Drop columns that require additional processing\n",
        "    data = data.drop(['Name','Ticket','Cabin'], axis=1)\n",
        "    # Fill null values with the mean of the column\n",
        "    data.fillna(data.mean(), inplace=True)\n",
        "    if 'Survived' in data.columns:\n",
        "        labels = data['Survived']\n",
        "        X = data.drop(['Survived'], axis=1)\n",
        "        X = data_normalizer(X)\n",
        "        return X, labels\n",
        "    else:\n",
        "        return data_normalizer(data)\n",
        "\n",
        "# This should split the data into our features and our labels\n",
        "features, labels = format_data(data)\n",
        "features.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKJVulyQMr_7"
      },
      "source": [
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOwG4OnkNkKg"
      },
      "source": [
        "### Split on train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmSSToPqM8wS"
      },
      "source": [
        "# Split the data set into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=2, stratify=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvUtDxHSOF5G"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeTQgDnaOO8m"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqN5e2EcPoxe"
      },
      "source": [
        "### Prepare inputs for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDSXjTBSOQ82"
      },
      "source": [
        "# Format the data into PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train.values)\n",
        "X_test = torch.FloatTensor(X_test.values)\n",
        "y_train = torch.LongTensor(y_train.values)\n",
        "y_test = torch.LongTensor(y_test.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YnqqVqaPrQf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_features):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_features, 270)\n",
        "        self.bn1 = nn.BatchNorm1d(270)\n",
        "        self.fc2 = nn.Linear(270, 50)\n",
        "        self.bn2 = nn.BatchNorm1d(50)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.dropout(x, p=0.1) \n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGrR9Q45TL3q"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIAYU-j3SNcd"
      },
      "source": [
        "model = Model(X_train.shape[1]).to(device)\n",
        "print(model)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, betas=(0.9, 0.99))\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kzjASJ5U6-A"
      },
      "source": [
        "# При данных зачениях (и те, что закомментированы), значение accuracy равняется 0.83\n",
        "batch_size = 25\n",
        "num_epochs = 20\n",
        "learning_rate = 0.02\n",
        "#batch_size = 39\n",
        "#num_epochs = 20\n",
        "#learning_rate = 0.05\n",
        "#batch_size = 90\n",
        "#num_epochs = 90\n",
        "#learning_rate = 0.1\n",
        "batch_no = len(X_train) // batch_size\n",
        "print(batch_no)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP0hZkMCcBB2"
      },
      "source": [
        "train_loss = np.zeros((num_epochs*batch_no,))\n",
        "train_accuracy = np.zeros((num_epochs*batch_no,))\n",
        "valid_loss = np.zeros((num_epochs*batch_no,))\n",
        "valid_accuracy = np.zeros((num_epochs*batch_no,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGKuJatviSaW"
      },
      "source": [
        "import torch.nn as nn\n",
        "loss_fn   = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpD31o8dTw5O"
      },
      "source": [
        "p=0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch {}'.format(epoch+1))\n",
        "    # x_train, y_train = shuffle(X_train, y_train)\n",
        "    x_train = X_train.to(device) # needs assignment\n",
        "    y_train = y_train.to(device) # needs assignment\n",
        "    # Mini batch learning\n",
        "    for i in range(batch_no):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        x_var = x_train[start:end]\n",
        "        y_var = y_train[start:end]\n",
        "        #Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x_var)\n",
        "        loss =criterion(pred, y_var)\n",
        "\n",
        "        train_loss[p] = loss.item()\n",
        "        train_correct = (torch.argmax(pred, dim=1) == y_var).type(torch.FloatTensor)\n",
        "        train_accuracy[p] = train_correct.mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        p+=1\n",
        "        with torch.no_grad():\n",
        "          y_pred = model(X_test)\n",
        "          loss = loss_fn(y_pred, y_test)\n",
        "          valid_loss[epoch] = loss.item()\n",
        "          correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "          valid_accuracy[epoch] = correct.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph7pYFVFT5Ye"
      },
      "source": [
        "# Evaluate the model\n",
        "test_var =  X_test.to(device) # needs assignment \n",
        "with torch.no_grad():\n",
        "    result = model(test_var)\n",
        "values, labels = torch.max(result, 1)\n",
        "num_right = np.sum(labels.data.cpu().numpy() == y_test.cpu().numpy())\n",
        "print('Accuracy {:.2f}'.format(num_right / len(y_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqEiQT3DmIlO"
      },
      "source": [
        "fig, [ax1, ax2] = plt.subplots(2, figsize=[12, 6], sharex=True)\n",
        "\n",
        "ax1.plot(train_accuracy)\n",
        "ax1.set_ylabel('train_accuracy')\n",
        "ax2.plot(train_loss)\n",
        "ax2.set_ylabel('train_loss')\n",
        "ax2.set_xlabel(\"epochs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTC0zfayhPUj"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "ax1.plot(valid_accuracy)\n",
        "ax1.set_ylabel(\"valid_accuracy\")\n",
        "ax2.plot(valid_loss)\n",
        "ax2.set_ylabel(\"valid_loss\")\n",
        "ax2.set_xlabel(\"epochs\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z37eZHMsjOfC"
      },
      "source": [
        "y_pred = model(X_test)\n",
        "y_hat=torch.argmax(y_pred, dim=1)\n",
        "len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztw1V0vdmSZe"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "# confusion matrix\n",
        "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
        "                  (\"Normalized confusion matrix\", 'true')]\n",
        "class_names=[   \"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf=confusion_matrix(y_test, y_hat)\n",
        "conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1TjSk0BeqVe"
      },
      "source": [
        "\n",
        "# Precision and recall\n",
        "from sklearn.metrics import precision_score, recall_score,f1_score\n",
        "print(f\"precision: {precision_score(y_test, y_hat, average='weighted')}\")\n",
        "print(f\"recall: {recall_score(y_test, y_hat, average='weighted')}\")\n",
        "print(f\"f1 score: {f1_score(y_test, y_hat, average='weighted')}\")\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_hat,)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_hat,  average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_hat, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_hat, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_hat, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_hat, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_hat, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_hat,average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_hat, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_hat, average='weighted')))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(y_test, y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV8avSSFfJn7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}