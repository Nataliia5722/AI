{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC2-4"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Homework_word_embedding.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QymD__LvsWF"
      },
      "source": [
        "###NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynXy3wlEplyo"
      },
      "source": [
        "In this homework, you will practice how to compute word embeddings and use them for sentiment analysis.\n",
        "\n",
        "By completing this homework you will:\n",
        "\n",
        "- Train word vectors from scratch.\n",
        "- Learn how to create batches of data.\n",
        "- Understand how backpropagation works.\n",
        "- Plot and visualize your learned word vectors.\n",
        "\n",
        "Knowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqpk9-Nzplyt",
        "outputId": "97d13a89-7aac-4a8e-af21-0dfadbb5f3f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Import Python libraries and helper functions (in utils2) \n",
        "import pandas as pd\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "!wget -q https://raw.githubusercontent.com/Nataliia5722/AI/main/NLP/utils2.py -O utils2.py > txt.log\n",
        "from utils2 import sigmoid, get_batches, compute_pca, get_dict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C_LNIhqplyu"
      },
      "source": [
        "# Download sentence tokenizer\n",
        "nltk.data.path.append('.')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwSo84rB0sGI",
        "outputId": "17e37c97-ce0e-4f6d-87d9-3c6c498baff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/Nataliia5722/AI/main/NLP/shakespeare.txt'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-27 17:31:08--  https://raw.githubusercontent.com/Nataliia5722/AI/main/NLP/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 306996 (300K) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>] 299.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-04-27 17:31:08 (7.92 MB/s) - ‘shakespeare.txt’ saved [306996/306996]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TJn9qX3plyv",
        "outputId": "859c6f90-d1c1-4837-f5a2-ca26003198ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load, tokenize and process the data\n",
        "import re                                                           #  Load the Regex-modul\n",
        "# data = pd.read_csv('https://raw.githubusercontent.com/Nataliia5722/AI/main/NLP/shakespeare.txt', sep='\\t', comment='#')       \n",
        "# data = data.replace(r'[,!?;-]', '',regex=True).astype(str)                                          #  Read in the data\n",
        "with open('shakespeare.txt') as f:\n",
        "    data = f.read()                                                 #  Read in the data\n",
        "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
        "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])                #  print data sample"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens: 60933 \n",
            " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2exY4S8plyv",
        "outputId": "3248a5a2-d761-46d6-9d30-c8a906300192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "print(\"Size of vocabulary: \",len(fdist) )\n",
        "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary:  5772\n",
            "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1252), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('you', 748), ('a', 742), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0foDaUyplyv"
      },
      "source": [
        "#### Mapping words to indices and indices to words\n",
        "We provide a helper function to create a dictionary that maps words to indices and indices to words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRB18xQqplyv",
        "outputId": "7d99f713-3fdd-4e15-b5c2-2d1f9be15127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary:  5772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yXPNzU9plyw",
        "outputId": "62a28b2b-dbe6-494d-dfae-29fe2c7c150a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# example of word to index mapping\n",
        "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
        "print(\"Word which has index 2743:  \",Ind2word[2743] )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index of the word 'king' :   2743\n",
            "Word which has index 2743:   king\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHUyP7sGplyw"
      },
      "source": [
        "<a name='2'></a>\n",
        "# 2 Training the Model\n",
        "\n",
        "###  Initializing the model\n",
        "\n",
        "You will now initialize two matrices and two vectors. \n",
        "- The first matrix ($W_1$) is of dimension $N \\times V$, where $V$ is the number of words in your vocabulary and $N$ is the dimension of your word vector.\n",
        "- The second matrix ($W_2$) is of dimension $V \\times N$. \n",
        "- Vector $b_1$ has dimensions $N\\times 1$\n",
        "- Vector $b_2$ has dimensions  $V\\times 1$. \n",
        "- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n",
        "\n",
        "The overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n",
        "\n",
        "<a name='ex-01'></a>\n",
        "### Exercise 01\n",
        "Please use [numpy.random.rand](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.\n",
        "\n",
        "**Note:** In the next cell you will encounter a random seed. Please **DO NOT** modify this seed so your solution can be tested correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tex2K0bYplyw"
      },
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: initialize_model\n",
        "def initialize_model(N,V, random_seed=1):\n",
        "    '''\n",
        "    Inputs: \n",
        "        N:  dimension of hidden vector \n",
        "        V:  dimension of vocabulary\n",
        "        random_seed: random seed for consistent results in the unit tests\n",
        "     Outputs: \n",
        "        W1, W2, b1, b2: initialized weights and biases\n",
        "    '''\n",
        "    \n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # W1 has shape (N,V)\n",
        "    W1 = np.random.rand(N, V)\n",
        "    # W2 has shape (V,N)\n",
        "    W2 = np.random.rand(V, N)\n",
        "    # b1 has shape (N,1)\n",
        "    b1 = np.random.rand(N, 1)\n",
        "    # b2 has shape (V,1)\n",
        "    b2 = np.random.rand(V, 1)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return W1, W2, b1, b2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yMCiP8Wplyx",
        "outputId": "fdbd7619-93d5-4750-86eb-f99de6600ee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test your function example.\n",
        "tmp_N = 4\n",
        "tmp_V = 10\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
        "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
        "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tmp_W1.shape: (4, 10)\n",
            "tmp_W2.shape: (10, 4)\n",
            "tmp_b1.shape: (4, 1)\n",
            "tmp_b2.shape: (10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoXFAlhiplyx"
      },
      "source": [
        "##### Expected Output \n",
        "\n",
        "```CPP\n",
        "tmp_W1.shape: (4, 10)\n",
        "tmp_W2.shape: (10, 4)\n",
        "tmp_b1.shape: (4, 1)\n",
        "tmp_b2.shape: (10, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7QnIVTDplyx"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 Softmax\n",
        "Before we can start training the model, we need to implement the softmax function as defined in equation 5:  \n",
        "\n",
        "<br>\n",
        "$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} $$\n",
        "\n",
        "- Array indexing in code starts at 0.\n",
        "- $V$ is the number of words in the vocabulary (which is also the number of rows of $z$).\n",
        "- $i$ goes from 0 to |V| - 1.\n",
        "\n",
        "\n",
        "<a name='ex-02'></a>\n",
        "### Exercise 02\n",
        "**Instructions**: Implement the softmax function below. \n",
        "\n",
        "- Assume that the input $z$ to `softmax` is a 2D array\n",
        "- Each training example is represented by a column of shape (V, 1) in this 2D array.\n",
        "- There may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency.  Let's call the batch size lowercase $m$, so the $z$ array has shape (V, m)\n",
        "- When taking the sum from $i=1 \\cdots V-1$, take the sum for each column (each example) separately.\n",
        "\n",
        "Please use\n",
        "- numpy.exp\n",
        "- numpy.sum (set the axis so that you take the sum of each column in z)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw9dDnGhplyy"
      },
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: softmax\n",
        "def softmax(z):\n",
        "    '''\n",
        "    Inputs: \n",
        "        z: output scores from the hidden layer\n",
        "    Outputs: \n",
        "        yhat: prediction (estimate of y)\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n",
        "    \n",
        "    # Calculate yhat (softmax)\n",
        "    e_z = np.exp(z)\n",
        "    yhat = e_z/np.sum(e_z, axis=0)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return yhat"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcYv3Y9splyy",
        "outputId": "cd8ffc8d-d7ee-44b6-bc91-26a7a3d738fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Test the function\n",
        "tmp = np.array([[1,2,3],\n",
        "                [1,1,1]\n",
        "               ])\n",
        "tmp_sm = softmax(tmp)\n",
        "display(tmp_sm)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.5       , 0.73105858, 0.88079708],\n",
              "       [0.5       , 0.26894142, 0.11920292]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmyiYWKWplyy"
      },
      "source": [
        "##### Expected Ouput\n",
        "\n",
        "```CPP\n",
        "array([[0.5       , 0.73105858, 0.88079708],\n",
        "       [0.5       , 0.26894142, 0.11920292]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mbo5m5splyz"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 Forward propagation\n",
        "\n",
        "<a name='ex-03'></a>\n",
        "### Exercise 03\n",
        "Implement the forward propagation $z$ according to equations (1) to (3). <br>\n",
        "\n",
        "\\begin{align}\n",
        " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
        " a &= ReLU(h)  \\tag{2} \\\\\n",
        " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
        "\\end{align}\n",
        "\n",
        "For that, you will use as activation the Rectified Linear Unit (ReLU) given by:\n",
        "\n",
        "$$f(h)=\\max (0,h) \\tag{6}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyjFFmGkplyz"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>You can use numpy.maximum(x1,x2) to get the maximum of two values</li>\n",
        "    <li>Use numpy.dot(A,B) to matrix multiply A and B</li>\n",
        "</ul>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swCiTVlKplyz"
      },
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: forward_prop\n",
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    '''\n",
        "    Inputs: \n",
        "        x:  average one hot vector for the context \n",
        "        W1, W2, b1, b2:  matrices and biases to be learned\n",
        "     Outputs: \n",
        "        z:  output score vector\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n",
        "    # Calculate h\n",
        "    h = np.dot(W1, x)+b1\n",
        "    # Apply the relu on h (store result in h(a))\n",
        "    #h = np.max(0, h)\n",
        "    # Calculate z\n",
        "    z = np.dot(W2, h)+b2\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return z, h"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKqXH5KZplyz",
        "outputId": "2f3edd91-4740-4549-9875-a758bd840e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test the function\n",
        "\n",
        "# Create some inputs\n",
        "tmp_N = 2\n",
        "tmp_V = 3\n",
        "tmp_x = np.array([[0,1,0]]).T\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
        "\n",
        "print(f\"x has shape {tmp_x.shape}\")\n",
        "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
        "\n",
        "# call function\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(\"call forward_prop\")\n",
        "print()\n",
        "# Look at output\n",
        "print(f\"z has shape {tmp_z.shape}\")\n",
        "print(\"z has values:\")\n",
        "print(tmp_z)\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"h has shape {tmp_h.shape}\")\n",
        "print(\"h has values:\")\n",
        "print(tmp_h)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x has shape (3, 1)\n",
            "N is 2 and vocabulary size V is 3\n",
            "call forward_prop\n",
            "\n",
            "z has shape (3, 1)\n",
            "z has values:\n",
            "[[0.55379268]\n",
            " [1.58960774]\n",
            " [1.50722933]]\n",
            "\n",
            "h has shape (2, 1)\n",
            "h has values:\n",
            "[[0.92477674]\n",
            " [1.02487333]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeACcHkAplyz"
      },
      "source": [
        "##### Expected output\n",
        "```CPP\n",
        "x has shape (3, 1)\n",
        "N is 2 and vocabulary size V is 3\n",
        "call forward_prop\n",
        "\n",
        "z has shape (3, 1)\n",
        "z has values:\n",
        "[[0.55379268]\n",
        " [1.58960774]\n",
        " [1.50722933]]\n",
        "\n",
        "h has shape (2, 1)\n",
        "h has values:\n",
        "[[0.92477674]\n",
        " [1.02487333]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppP1e6T4ply0"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "## 2.3 Cost function\n",
        "\n",
        "- We have implemented the *cross-entropy* cost function for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcYoBCX3ply0"
      },
      "source": [
        "# compute_cost: cross-entropy cost functioN\n",
        "def compute_cost(y, yhat, batch_size):\n",
        "    # cost function \n",
        "    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n",
        "    cost = - 1/batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igzYKzzgply0",
        "outputId": "f5ebf1e0-85fb-475d-96d4-e93b6eb5d921",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test the function\n",
        "tmp_C = 2\n",
        "tmp_N = 50\n",
        "tmp_batch_size = 4\n",
        "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
        "tmp_V = len(word2Ind)\n",
        "\n",
        "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
        "        \n",
        "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
        "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
        "\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "\n",
        "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
        "\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
        "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
        "\n",
        "tmp_yhat = softmax(tmp_z)\n",
        "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
        "\n",
        "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
        "print(\"call compute_cost\")\n",
        "print(f\"tmp_cost {tmp_cost:.4f}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tmp_x.shape (5772, 4)\n",
            "tmp_y.shape (5772, 4)\n",
            "tmp_W1.shape (50, 5772)\n",
            "tmp_W2.shape (5772, 50)\n",
            "tmp_b1.shape (50, 1)\n",
            "tmp_b2.shape (5772, 1)\n",
            "tmp_z.shape: (5772, 4)\n",
            "tmp_h.shape: (50, 4)\n",
            "tmp_yhat.shape: (5772, 4)\n",
            "call compute_cost\n",
            "tmp_cost 12.9825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbCUNFeCply0"
      },
      "source": [
        "##### Expected output\n",
        "\n",
        "```CPP\n",
        "tmp_x.shape (5778, 4)\n",
        "tmp_y.shape (5778, 4)\n",
        "tmp_W1.shape (50, 5778)\n",
        "tmp_W2.shape (5778, 50)\n",
        "tmp_b1.shape (50, 1)\n",
        "tmp_b2.shape (5778, 1)\n",
        "tmp_z.shape: (5778, 4)\n",
        "tmp_h.shape: (50, 4)\n",
        "tmp_yhat.shape: (5778, 4)\n",
        "call compute_cost\n",
        "tmp_cost 9.9560\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERyyFYNtply1"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "## 2.4 Training the Model - Backpropagation\n",
        "\n",
        "<a name='ex-04'></a>\n",
        "### Exercise 04\n",
        "Now that you have understood how the CBOW model works, you will train it. <br>\n",
        "You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWzU8Jwvply1"
      },
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: back_prop\n",
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
        "    '''\n",
        "    Inputs: \n",
        "        x:  average one hot vector for the context \n",
        "        yhat: prediction (estimate of y)\n",
        "        y:  target vector\n",
        "        h:  hidden vector (see eq. 1)\n",
        "        W1, W2, b1, b2:  matrices and biases  \n",
        "        batch_size: batch size \n",
        "     Outputs: \n",
        "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
        "    '''\n",
        "    ### START CODE HERE (Replace instanes of 'None' with your code) ###\n",
        "    \n",
        "    # Compute l1 as W2^T (Yhat - Y)\n",
        "    # Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n",
        "    l1 = (yhat-y)\n",
        "    # Apply relu to l1\n",
        "    #l1 = None\n",
        "    # Compute the gradient of W1\n",
        "    grad_W1 = np.gradient(W1, axis=0)\n",
        "    # Compute the gradient of W2\n",
        "    grad_W2 = np.gradient(W2, axis=0)\n",
        "    # Compute the gradient of b1\n",
        "    grad_b1 = np.gradient(b1, axis=0)\n",
        "    # Compute the gradient of b2\n",
        "    grad_b2 = np.gradient(b2, axis=0)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7NCVrSmply1",
        "outputId": "eec82076-3359-47a1-92bb-015507535172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test the function\n",
        "tmp_C = 2\n",
        "tmp_N = 50\n",
        "tmp_batch_size = 4\n",
        "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
        "tmp_V = len(word2Ind)\n",
        "\n",
        "# get a batch of data\n",
        "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
        "\n",
        "print(\"get a batch of data\")\n",
        "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
        "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Initialize weights and biases\")\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "\n",
        "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Forwad prop to get z and h\")\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
        "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Get yhat by calling softmax\")\n",
        "tmp_yhat = softmax(tmp_z)\n",
        "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
        "\n",
        "tmp_m = (2*tmp_C)\n",
        "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
        "\n",
        "print()\n",
        "print(\"call back_prop\")\n",
        "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
        "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
        "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
        "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "get a batch of data\n",
            "tmp_x.shape (5772, 4)\n",
            "tmp_y.shape (5772, 4)\n",
            "\n",
            "Initialize weights and biases\n",
            "tmp_W1.shape (50, 5772)\n",
            "tmp_W2.shape (5772, 50)\n",
            "tmp_b1.shape (50, 1)\n",
            "tmp_b2.shape (5772, 1)\n",
            "\n",
            "Forwad prop to get z and h\n",
            "tmp_z.shape: (5772, 4)\n",
            "tmp_h.shape: (50, 4)\n",
            "\n",
            "Get yhat by calling softmax\n",
            "tmp_yhat.shape: (5772, 4)\n",
            "\n",
            "call back_prop\n",
            "tmp_grad_W1.shape (50, 5772)\n",
            "tmp_grad_W2.shape (5772, 50)\n",
            "tmp_grad_b1.shape (50, 1)\n",
            "tmp_grad_b2.shape (5772, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHmg5CFNply1"
      },
      "source": [
        "##### Expected output\n",
        "\n",
        "```CPP\n",
        "get a batch of data\n",
        "tmp_x.shape (5778, 4)\n",
        "tmp_y.shape (5778, 4)\n",
        "\n",
        "Initialize weights and biases\n",
        "tmp_W1.shape (50, 5778)\n",
        "tmp_W2.shape (5778, 50)\n",
        "tmp_b1.shape (50, 1)\n",
        "tmp_b2.shape (5778, 1)\n",
        "\n",
        "Forwad prop to get z and h\n",
        "tmp_z.shape: (5778, 4)\n",
        "tmp_h.shape: (50, 4)\n",
        "\n",
        "Get yhat by calling softmax\n",
        "tmp_yhat.shape: (5778, 4)\n",
        "\n",
        "call back_prop\n",
        "tmp_grad_W1.shape (50, 5778)\n",
        "tmp_grad_W2.shape (5778, 50)\n",
        "tmp_grad_b1.shape (50, 1)\n",
        "tmp_grad_b2.shape (5778, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C5CG9niply2"
      },
      "source": [
        "<a name='2.5'></a>\n",
        "## Gradient Descent\n",
        "\n",
        "<a name='ex-05'></a>\n",
        "### Exercise 05\n",
        "Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set. \n",
        "\n",
        "**Hint:** For that, you will use `initialize_model` and the `back_prop` functions which you just created (and the `compute_cost` function). You can also use the provided `get_batches` helper function:\n",
        "\n",
        "```for x, y in get_batches(data, word2Ind, V, C, batch_size):```\n",
        "\n",
        "```...```\n",
        "\n",
        "Also: print the cost after each batch is processed (use batch size = 128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6cIqOJ1ply2"
      },
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: gradient_descent\n",
        "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n",
        "    \n",
        "    '''\n",
        "    This is the gradient_descent function\n",
        "    \n",
        "      Inputs: \n",
        "        data:      text\n",
        "        word2Ind:  words to Indices\n",
        "        N:         dimension of hidden vector  \n",
        "        V:         dimension of vocabulary \n",
        "        num_iters: number of iterations  \n",
        "     Outputs: \n",
        "        W1, W2, b1, b2:  updated matrices and biases   \n",
        "\n",
        "    '''\n",
        "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n",
        "    batch_size = 128\n",
        "    iters = 0\n",
        "    C = 2\n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "        ### START CODE HERE (Replace instances of 'None' with your own code) ###\n",
        "        # Get z and h\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
        "        # Get yhat\n",
        "        yhat = softmax(z)\n",
        "        print(yhat)\n",
        "        # Get cost\n",
        "        cost = compute_cost(y, yhat, batch_size)\n",
        "        if ( (iters+1) % 10 == 0):\n",
        "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "        # Get gradients\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
        "        \n",
        "        # Update weights and biases\n",
        "        W1 -= grad_W1 \n",
        "        W2 -= grad_W2\n",
        "        b1 -= grad_b1\n",
        "        b2 -= grad_b2\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        iters += 1 \n",
        "        if iters == num_iters: \n",
        "            break\n",
        "        if iters % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "            \n",
        "    return W1, W2, b1, b2"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQTs90jBply2",
        "outputId": "54a7a413-1992-4b4c-a2ef-083d0059541f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test your function\n",
        "C = 2\n",
        "N = 50\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "num_iters = 150\n",
        "print(\"Call gradient_descent\")\n",
        "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Call gradient_descent\n",
            "[[2.52232524e-05 2.52232524e-05 2.52232524e-05 ... 2.52232524e-05\n",
            "  2.52232524e-05 2.52232524e-05]\n",
            " [5.07185189e-06 5.07185189e-06 5.07185189e-06 ... 5.07185189e-06\n",
            "  5.07185189e-06 5.07185189e-06]\n",
            " [5.98078915e-06 5.98078915e-06 5.98078915e-06 ... 5.98078915e-06\n",
            "  5.98078915e-06 5.98078915e-06]\n",
            " ...\n",
            " [8.24780305e-05 8.24780305e-05 8.24780305e-05 ... 8.24780305e-05\n",
            "  8.24780305e-05 8.24780305e-05]\n",
            " [3.05785822e-04 3.05785822e-04 3.05785822e-04 ... 3.05785822e-04\n",
            "  3.05785822e-04 3.05785822e-04]\n",
            " [1.25455529e-03 1.25455529e-03 1.25455529e-03 ... 1.25455529e-03\n",
            "  1.25455529e-03 1.25455529e-03]]\n",
            "[[6.16964854e-05 6.16964854e-05 6.16964854e-05 ... 6.16964854e-05\n",
            "  6.16964854e-05 6.16964854e-05]\n",
            " [1.44037665e-06 1.44037665e-06 1.44037665e-06 ... 1.44037665e-06\n",
            "  1.44037665e-06 1.44037665e-06]\n",
            " [3.15532952e-07 3.15532952e-07 3.15532952e-07 ... 3.15532952e-07\n",
            "  3.15532952e-07 3.15532952e-07]\n",
            " ...\n",
            " [5.90482194e-05 5.90482194e-05 5.90482194e-05 ... 5.90482194e-05\n",
            "  5.90482194e-05 5.90482194e-05]\n",
            " [1.25990909e-05 1.25990909e-05 1.25990909e-05 ... 1.25990909e-05\n",
            "  1.25990909e-05 1.25990909e-05]\n",
            " [4.19659425e-05 4.19659425e-05 4.19659425e-05 ... 4.19659425e-05\n",
            "  4.19659425e-05 4.19659425e-05]]\n",
            "[[7.45136009e-03 7.45136009e-03 7.45136009e-03 ... 7.45136009e-03\n",
            "  7.45136009e-03 7.45136009e-03]\n",
            " [9.70677784e-07 9.70677784e-07 9.70677784e-07 ... 9.70677784e-07\n",
            "  9.70677784e-07 9.70677784e-07]\n",
            " [4.09325677e-09 4.09325677e-09 4.09325677e-09 ... 4.09325677e-09\n",
            "  4.09325677e-09 4.09325677e-09]\n",
            " ...\n",
            " [4.54283941e-05 4.54283941e-05 4.54283941e-05 ... 4.54283941e-05\n",
            "  4.54283941e-05 4.54283941e-05]\n",
            " [3.25553030e-06 3.25553030e-06 3.25553030e-06 ... 3.25553030e-06\n",
            "  3.25553030e-06 3.25553030e-06]\n",
            " [1.83380960e-06 1.83380960e-06 1.83380960e-06 ... 1.83380960e-06\n",
            "  1.83380960e-06 1.83380960e-06]]\n",
            "[[9.89687024e-01 9.89687024e-01 9.89687024e-01 ... 9.89687024e-01\n",
            "  9.89687024e-01 9.89687024e-01]\n",
            " [6.12544310e-08 6.12544310e-08 6.12544310e-08 ... 6.12544310e-08\n",
            "  6.12544310e-08 6.12544310e-08]\n",
            " [4.23208779e-15 4.23208779e-15 4.23208779e-15 ... 4.23208779e-15\n",
            "  4.23208779e-15 4.23208779e-15]\n",
            " ...\n",
            " [1.53088983e-09 1.53088983e-09 1.53088983e-09 ... 1.53088983e-09\n",
            "  1.53088983e-09 1.53088983e-09]\n",
            " [6.15365998e-10 6.15365998e-10 6.15365998e-10 ... 6.15365998e-10\n",
            "  6.15365998e-10 6.15365998e-10]\n",
            " [2.68577840e-10 2.68577840e-10 2.68577840e-10 ... 2.68577840e-10\n",
            "  2.68577840e-10 2.68577840e-10]]\n",
            "[[9.99993174e-01 9.99993174e-01 9.99993174e-01 ... 9.99993174e-01\n",
            "  9.99993174e-01 9.99993174e-01]\n",
            " [1.52838755e-08 1.52838755e-08 1.52838755e-08 ... 1.52838755e-08\n",
            "  1.52838755e-08 1.52838755e-08]\n",
            " [7.07486365e-22 7.07486365e-22 7.07486365e-22 ... 7.07486365e-22\n",
            "  7.07486365e-22 7.07486365e-22]\n",
            " ...\n",
            " [1.55432489e-16 1.55432489e-16 1.55432489e-16 ... 1.55432489e-16\n",
            "  1.55432489e-16 1.55432489e-16]\n",
            " [2.81953539e-18 2.81953539e-18 2.81953539e-18 ... 2.81953539e-18\n",
            "  2.81953539e-18 2.81953539e-18]\n",
            " [3.56203216e-18 3.56203216e-18 3.56203216e-18 ... 3.56203216e-18\n",
            "  3.56203216e-18 3.56203216e-18]]\n",
            "[[1.29342096e-08 1.29342096e-08 1.29342096e-08 ... 1.29342096e-08\n",
            "  1.29342096e-08 1.29342096e-08]\n",
            " [8.40438503e-10 8.40438503e-10 8.40438503e-10 ... 8.40438503e-10\n",
            "  8.40438503e-10 8.40438503e-10]\n",
            " [4.03671272e-19 4.03671272e-19 4.03671272e-19 ... 4.03671272e-19\n",
            "  4.03671272e-19 4.03671272e-19]\n",
            " ...\n",
            " [1.75201353e-14 1.75201353e-14 1.75201353e-14 ... 1.75201353e-14\n",
            "  1.75201353e-14 1.75201353e-14]\n",
            " [3.97262452e-23 3.97262452e-23 3.97262452e-23 ... 3.97262452e-23\n",
            "  3.97262452e-23 3.97262452e-23]\n",
            " [2.73208735e-25 2.73208735e-25 2.73208735e-25 ... 2.73208735e-25\n",
            "  2.73208735e-25 2.73208735e-25]]\n",
            "[[3.12830495e-46 3.12830495e-46 3.12830495e-46 ... 3.12830495e-46\n",
            "  3.12830495e-46 3.12830495e-46]\n",
            " [3.84697857e-38 3.84697857e-38 3.84697857e-38 ... 3.84697857e-38\n",
            "  3.84697857e-38 3.84697857e-38]\n",
            " [2.15710353e-24 2.15710353e-24 2.15710353e-24 ... 2.15710353e-24\n",
            "  2.15710353e-24 2.15710353e-24]\n",
            " ...\n",
            " [7.19957962e-13 7.19957962e-13 7.19957962e-13 ... 7.19957962e-13\n",
            "  7.19957962e-13 7.19957962e-13]\n",
            " [1.32561963e-27 1.32561963e-27 1.32561963e-27 ... 1.32561963e-27\n",
            "  1.32561963e-27 1.32561963e-27]\n",
            " [7.66616683e-37 7.66616683e-37 7.66616683e-37 ... 7.66616683e-37\n",
            "  7.66616683e-37 7.66616683e-37]]\n",
            "[[2.05100039e-102 2.05100039e-102 2.05100039e-102 ... 2.05100039e-102\n",
            "  2.05100039e-102 2.05100039e-102]\n",
            " [1.22651254e-093 1.22651254e-093 1.22651254e-093 ... 1.22651254e-093\n",
            "  1.22651254e-093 1.22651254e-093]\n",
            " [5.81056381e-058 5.81056381e-058 5.81056381e-058 ... 5.81056381e-058\n",
            "  5.81056381e-058 5.81056381e-058]\n",
            " ...\n",
            " [2.53227046e-030 2.53227046e-030 2.53227046e-030 ... 2.53227046e-030\n",
            "  2.53227046e-030 2.53227046e-030]\n",
            " [1.22660247e-036 1.22660247e-036 1.22660247e-036 ... 1.22660247e-036\n",
            "  1.22660247e-036 1.22660247e-036]\n",
            " [5.65085165e-051 5.65085165e-051 5.65085165e-051 ... 5.65085165e-051\n",
            "  5.65085165e-051 5.65085165e-051]]\n",
            "[[1.68751816e-141 1.68751816e-141 1.68751816e-141 ... 1.68751816e-141\n",
            "  1.68751816e-141 1.68751816e-141]\n",
            " [3.49701854e-128 3.49701854e-128 3.49701854e-128 ... 3.49701854e-128\n",
            "  3.49701854e-128 3.49701854e-128]\n",
            " [3.39653578e-092 3.39653578e-092 3.39653578e-092 ... 3.39653578e-092\n",
            "  3.39653578e-092 3.39653578e-092]\n",
            " ...\n",
            " [5.56979185e-079 5.56979185e-079 5.56979185e-079 ... 5.56979185e-079\n",
            "  5.56979185e-079 5.56979185e-079]\n",
            " [1.33188670e-071 1.33188670e-071 1.33188670e-071 ... 1.33188670e-071\n",
            "  1.33188670e-071 1.33188670e-071]\n",
            " [1.77577313e-080 1.77577313e-080 1.77577313e-080 ... 1.77577313e-080\n",
            "  1.77577313e-080 1.77577313e-080]]\n",
            "[[3.02469399e-228 3.02469399e-228 3.02469399e-228 ... 3.02469399e-228\n",
            "  3.02469399e-228 3.02469399e-228]\n",
            " [2.08827994e-169 2.08827994e-169 2.08827994e-169 ... 2.08827994e-169\n",
            "  2.08827994e-169 2.08827994e-169]\n",
            " [7.99631351e-082 7.99631351e-082 7.99631351e-082 ... 7.99631351e-082\n",
            "  7.99631351e-082 7.99631351e-082]\n",
            " ...\n",
            " [2.93974425e-167 2.93974425e-167 2.93974425e-167 ... 2.93974425e-167\n",
            "  2.93974425e-167 2.93974425e-167]\n",
            " [3.36967663e-187 3.36967663e-187 3.36967663e-187 ... 3.36967663e-187\n",
            "  3.36967663e-187 3.36967663e-187]\n",
            " [2.77434801e-196 2.77434801e-196 2.77434801e-196 ... 2.77434801e-196\n",
            "  2.77434801e-196 2.77434801e-196]]\n",
            "iters: 10 cost: nan\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 20 cost: nan\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "iters: 30 cost: nan\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 40 cost: nan\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "iters: 50 cost: nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 60 cost: nan\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "iters: 70 cost: nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 80 cost: nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "iters: 90 cost: nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 100 cost: nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "iters: 110 cost: nan\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 120 cost: nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "iters: 130 cost: nan\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 140 cost: nan\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n",
            "iters: 150 cost: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oKH9E3Mply2"
      },
      "source": [
        "##### Expected Output\n",
        "\n",
        "\n",
        "```CPP\n",
        "iters: 10 cost: 0.789141\n",
        "iters: 20 cost: 0.105543\n",
        "iters: 30 cost: 0.056008\n",
        "iters: 40 cost: 0.038101\n",
        "iters: 50 cost: 0.028868\n",
        "iters: 60 cost: 0.023237\n",
        "iters: 70 cost: 0.019444\n",
        "iters: 80 cost: 0.016716\n",
        "iters: 90 cost: 0.014660\n",
        "iters: 100 cost: 0.013054\n",
        "iters: 110 cost: 0.012133\n",
        "iters: 120 cost: 0.011370\n",
        "iters: 130 cost: 0.010698\n",
        "iters: 140 cost: 0.010100\n",
        "iters: 150 cost: 0.009566\n",
        "```\n",
        "\n",
        "Your numbers may differ a bit depending on which version of Python you're using."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMd_BiYGply3"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3.0 Visualizing the word vectors\n",
        "\n",
        "In this part you will visualize the word vectors trained using the function you just coded above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ardsu54ply3"
      },
      "source": [
        "# visualizing the word vectors here\n",
        "from matplotlib import pyplot\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "words = ['king', 'queen','lord','man', 'woman','dog','horse',\n",
        "         'rich','happy','sad']\n",
        "\n",
        "embs = (W1.T + W2)/2.0\n",
        " \n",
        "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
        "idx = [word2Ind[word] for word in words]\n",
        "X = embs[idx, :]\n",
        "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMULgN8Uply3"
      },
      "source": [
        "result= compute_pca(X, 2)\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX8OvyiLply3"
      },
      "source": [
        "You can see that woman and queen are next to each other. However, we have to be careful with the interpretation of this projected word vectors, since the PCA depends on the projection -- as shown in the following illustration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IGTWYoQply4"
      },
      "source": [
        "result= compute_pca(X, 4)\n",
        "pyplot.scatter(result[:, 3], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEhxLXrzply4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}